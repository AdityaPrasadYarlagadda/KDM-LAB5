# -*- coding: utf-8 -*-
"""Kdm-icp5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/110V0fb1_rRNDEBGnfNmLxAcjHZgAxOOM
"""

!pip install pyspark

from __future__ import print_function
from pyspark import SparkConf, SparkContext
from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from pyspark.sql import SparkSession
from pyspark.ml.feature import NGram
from pyspark.ml.feature import Word2Vec

"""**Importing 5 text files containing data**"""

with open("/content/para1.txt","r+") as f1:
  file1 = f1.read()
with open("/content/para2.txt","r+") as f2:
  file2 = f2.read()
with open("/content/para3.txt","r+") as f3:
    file3 = f3.read()
with open("/content/para4.txt","r+") as f4:
    file4 = f4.read()
with open("/content/para5.txt","r+") as f5:
    file5 = f5.read()

documents = [file1,file2,file3,file4,file5]

"""**printing all the data**"""

documents

"""**1.Finding Out top 10 Tf-idf words**"""

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus
vect = TfidfVectorizer()
#created TfidfVectorizer object
tfidf_matrix = vect.fit_transform(documents)
#passed list of documents or corpus to obt method fit_transform
df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())
# converted method output to panda data frame 
pd.set_option('display.max_columns', 20)

df.loc['Total'] = df.sum() # adding row to value total

#filtering values of words whos tfidf is greater than 0.3
# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version
print (df.T.sort_values('Total', ascending=True).tail(10).T)

"""**2.Finding out top 10 Tf-idf words for lemmatized input data**"""

import nltk;
nltk.download('punkt');
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

w1 = nltk.word_tokenize(file1)
w2 = nltk.word_tokenize(file2)
w3 = nltk.word_tokenize(file3)
w4 = nltk.word_tokenize(file4)
w5 = nltk.word_tokenize(file5)

lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in w1])
lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in w2])
lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in w3])
lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in w4])
lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in w5])

documents = [lemmatized_document1,lemmatized_document2,lemmatized_document3,lemmatized_document4,lemmatized_document5]

# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus
vect = TfidfVectorizer()
#created TfidfVectorizer object
tfidf_matrix = vect.fit_transform(documents)
#passed list of documents or corpus to obt method fit_transform
df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())
# converted method output to panda data frame 

df.loc['Total'] = df.sum() # adding row to value total

#filtering values of words whos tfidf is greater than 0.3
# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version
print (df.T.sort_values('Total', ascending=True).tail(10).T)

"""**3.Finding out top 10 TF-IDF words for N-gram based input data**"""

# this function takes document and n int value to generate list of n grams
def ngrams(input, n):
    input = input.split(' ')
    output = []
    for i in range(len(input)-n+1):
        output.append(input[i:i+n])
    return output

ngram_doc1 = ' '.join([' '.join(x) for x in ngrams(file1, 2)])
ngram_doc2 = ' '.join([' '.join(x) for x in ngrams(file2, 2)])
ngram_doc3 = ' '.join([' '.join(x) for x in ngrams(file3, 2)])
ngram_doc4 = ' '.join([' '.join(x) for x in ngrams(file4, 2)])
ngram_doc5 = ' '.join([' '.join(x) for x in ngrams(file5, 2)])

# documents = [ngram_doc1,ngram_doc2,ngram_doc3,ngram_doc4,ngram_doc5]

documents = [file1,file2,file3,file4,file5]

# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus
vect = TfidfVectorizer( ngram_range=(2,2)) # TfidfVectorizer has inbuilt ngram kwarg which show tfidf for ngrams
#created TfidfVectorizer object
tfidf_matrix = vect.fit_transform(documents)
#passed list of documents or corpus to obt method fit_transform
df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())
# converted method output to panda data frame 

df.loc['Total'] = df.sum() # adding row to value total

#filtering values of words whos tfidf is greater than 0.3
# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version
print (df.T.sort_values('Total', ascending=True).tail(10).T)

"""**2.Permorfing a spark program to read a dataset and find the W2V similar words (words with higher cosine similarity) for the Top10 TF-IDF Words**"""

from __future__ import print_function
from pyspark import SparkConf, SparkContext
from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from pyspark.sql import SparkSession
from pyspark.ml.feature import NGram
from pyspark.ml.feature import Word2Vec

# creating spark session
spark = SparkSession.builder.appName("TfIdf Example").getOrCreate()

documentData = spark.createDataFrame([
        (0.0, file1),
        (0.1, file2),
        (0.2, file3),
        (0.3, file4),
        (0.5, file5)
    ], ["label", "document"])

# creating tokens/words from the sentence data
tokenizer = Tokenizer(inputCol="document", outputCol="words")
wordsData = tokenizer.transform(documentData)
print (documentData)
wordsData.show()

"""**a.Performing a task without NLP**"""

# applying tf on the words data
hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=200)
tf = hashingTF.transform(wordsData)
# alternatively, CountVectorizer can also be used to get term frequency vectors
# calculating the IDF
tf.cache()
idf = IDF(inputCol="rawFeatures", outputCol="features")
idf = idf.fit(tf)
tfidf = idf.transform(tf)
#displaying the results
tfidf.select("label", "features").show()


print("TF-IDF without NLP:")
for each in tfidf.collect():
    print(each)
    print(each['rawFeatures'])
spark.stop()

"""**b.Performing the task with lemmitization**"""

import nltk;
nltk.download('punkt');
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

w1 = nltk.word_tokenize(file1)
w2 = nltk.word_tokenize(file2)
w3 = nltk.word_tokenize(file3)
w4 = nltk.word_tokenize(file4)
w5 = nltk.word_tokenize(file5)

lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in w1])
lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in w2])
lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in w3])
lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in w4])
lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in w5])

### lemmatizing words from 5 input docs same as previos task

# creating spark session
spark = SparkSession.builder.appName("TfIdf Example").getOrCreate()

documentData = spark.createDataFrame([
        (0.0, lemmatized_document1),
        (0.1, lemmatized_document2),
        (0.2, lemmatized_document3),
        (0.3, lemmatized_document4),
        (0.5, lemmatized_document5)
    ], ["label", "document"])

# creating tokens/words from the sentence data
tokenizer = Tokenizer(inputCol="document", outputCol="words")
wordsData = tokenizer.transform(documentData)
print (documentData)
wordsData.show()

# applying tf on the words data
hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=200)
tf = hashingTF.transform(wordsData)
# alternatively, CountVectorizer can also be used to get term frequency vectors
# calculating the IDF
tf.cache()
idf = IDF(inputCol="rawFeatures", outputCol="features")
idf = idf.fit(tf)
tfidf = idf.transform(tf)
#displaying the results
tfidf.select("label", "features").show()


print("TF-IDF with Lemmatization:")
for each in tfidf.collect():
    print(each)
    print(each['rawFeatures'])
spark.stop()

"""**c.Performing the task with N-grams data**"""

spark = SparkSession.builder.appName("TfIdf Example").getOrCreate()

documentData = spark.createDataFrame([
        (0.0, file1.split(' ')),
        (0.1, file2.split(' ')),
        (0.2, file3.split(' ')),
        (0.3, file4.split(' ')),
        (0.5, file5.split(' '))
    ], ["label", "document"])


ngram = NGram(n=2, inputCol="document", outputCol="ngrams")

ngramDataFrame = ngram.transform(documentData)

# applying tf on the words data
hashingTF = HashingTF(inputCol="ngrams", outputCol="rawFeatures", numFeatures=200)
tf = hashingTF.transform(ngramDataFrame)
# alternatively, CountVectorizer can also be used to get term frequency vectors
# calculating the IDF
tf.cache()
idf = IDF(inputCol="rawFeatures", outputCol="features")
idf = idf.fit(tf)
tfidf = idf.transform(tf)
#displaying the results
tfidf.select("label", "features").show()


print("TF-IDF with ngram:")
for each in tfidf.collect():
    print(each)
    print(each['rawFeatures'])
spark.stop()