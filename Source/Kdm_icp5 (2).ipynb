{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kdm-icp5",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIA6tLZz9uNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50f3dfb-cdd2-4e86-abf7-c16160918324"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 69kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 43.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=473ac55e23f829fbcb1e5a23a9d8fdc8f9fe85af99ed9de12cc10f9b9f50bd26\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWH93Nh_3dZs"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-3wbWaPozQu"
      },
      "source": [
        "**Importing 5 text files containing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OtshkPepOgL"
      },
      "source": [
        "with open(\"/content/para1.txt\",\"r+\") as f1:\r\n",
        "  file1 = f1.read()\r\n",
        "with open(\"/content/para2.txt\",\"r+\") as f2:\r\n",
        "  file2 = f2.read()\r\n",
        "with open(\"/content/para3.txt\",\"r+\") as f3:\r\n",
        "    file3 = f3.read()\r\n",
        "with open(\"/content/para4.txt\",\"r+\") as f4:\r\n",
        "    file4 = f4.read()\r\n",
        "with open(\"/content/para5.txt\",\"r+\") as f5:\r\n",
        "    file5 = f5.read()\r\n",
        "\r\n",
        "documents = [file1,file2,file3,file4,file5]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nh82Tadv9us"
      },
      "source": [
        "**printing all the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-LuZI3XwA2I",
        "outputId": "50f6f0cb-2bdc-4d04-ae84-1c1d29495bbc"
      },
      "source": [
        "documents"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The tech giant has blocked news to Australians on its platform since last Thursday amid a dispute over a proposed law which would force it and Google to pay news publishers for content.',\n",
              " 'The lessons that the piano teacher Cornelia Vertenstein taught her students also resounded with many others, including me.',\n",
              " 'Cornelia Vertenstein, a Holocaust survivor, gave her last piano lesson on Feb. She was not feeling well, so she arranged a ride to the hospital.',\n",
              " 'Pneumonia settled in, and family gathered, sensing the end of a quietly extraordinary life.',\n",
              " 'She began giving lessons at age in war-torn Romania. She did not stop for nearly years. Toward the end, adapting to the pandemic, Ms. Vertenstein gave lessons on FaceTime from her home in Denver.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrXqfPTHwPpX"
      },
      "source": [
        "**1.Finding Out top 10 Tf-idf words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jLBVZavwilu",
        "outputId": "f68cb354-8ca6-49c2-fe7c-870d012a23df"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "pd.set_option('display.max_columns', 20)\r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             on     piano  cornelia       her  vertenstein   lessons  \\\n",
            "0      0.121420  0.000000  0.000000  0.000000     0.000000  0.000000   \n",
            "1      0.000000  0.210014  0.210014  0.174331     0.174331  0.210014   \n",
            "2      0.154133  0.185682  0.185682  0.154133     0.154133  0.000000   \n",
            "3      0.000000  0.000000  0.000000  0.000000     0.000000  0.000000   \n",
            "4      0.118174  0.000000  0.000000  0.118174     0.118174  0.284725   \n",
            "Total  0.393726  0.395696  0.395696  0.446637     0.446637  0.494739   \n",
            "\n",
            "             to        in       she       the  \n",
            "0      0.242841  0.000000  0.000000  0.086392  \n",
            "1      0.000000  0.000000  0.000000  0.248076  \n",
            "2      0.154133  0.000000  0.371363  0.109667  \n",
            "3      0.000000  0.241293  0.000000  0.142512  \n",
            "4      0.118174  0.284725  0.284725  0.168163  \n",
            "Total  0.515147  0.526018  0.656088  0.754808  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p70mj3hVxl9z"
      },
      "source": [
        "**2.Finding out top 10 Tf-idf words for lemmatized input data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo1cvQkRyjBa",
        "outputId": "6c1e8d23-9c45-46f9-d363-e80a41eddce2"
      },
      "source": [
        "import nltk;\r\n",
        "nltk.download('punkt');\r\n",
        "nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "w1 = nltk.word_tokenize(file1)\r\n",
        "w2 = nltk.word_tokenize(file2)\r\n",
        "w3 = nltk.word_tokenize(file3)\r\n",
        "w4 = nltk.word_tokenize(file4)\r\n",
        "w5 = nltk.word_tokenize(file5)\r\n",
        "\r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in w1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in w2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in w3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in w4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in w5])\r\n",
        "\r\n",
        "documents = [lemmatized_document1,lemmatized_document2,lemmatized_document3,lemmatized_document4,lemmatized_document5]\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "             on     piano  cornelia       her  vertenstein        to  \\\n",
            "0      0.117616  0.000000  0.000000  0.000000     0.000000  0.235231   \n",
            "1      0.000000  0.211469  0.211469  0.175539     0.175539  0.000000   \n",
            "2      0.156434  0.188454  0.188454  0.156434     0.156434  0.156434   \n",
            "3      0.000000  0.000000  0.000000  0.000000     0.000000  0.000000   \n",
            "4      0.119692  0.000000  0.000000  0.119692     0.119692  0.119692   \n",
            "Total  0.393742  0.399924  0.399924  0.451665     0.451665  0.511358   \n",
            "\n",
            "             in    lesson       she       the  \n",
            "0      0.000000  0.000000  0.000000  0.083685  \n",
            "1      0.000000  0.175539  0.000000  0.249794  \n",
            "2      0.000000  0.156434  0.376909  0.111304  \n",
            "3      0.241293  0.000000  0.000000  0.142512  \n",
            "4      0.288383  0.239384  0.288383  0.170324  \n",
            "Total  0.529676  0.571357  0.665292  0.757619  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hU_fcdzzgq9"
      },
      "source": [
        "**3.Finding out top 10 TF-IDF words for N-gram based input data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PuFtf3Wz3UU",
        "outputId": "800df586-e4c3-4a97-c8aa-7bd0f83acffa"
      },
      "source": [
        "# this function takes document and n int value to generate list of n grams\r\n",
        "def ngrams(input, n):\r\n",
        "    input = input.split(' ')\r\n",
        "    output = []\r\n",
        "    for i in range(len(input)-n+1):\r\n",
        "        output.append(input[i:i+n])\r\n",
        "    return output\r\n",
        "\r\n",
        "ngram_doc1 = ' '.join([' '.join(x) for x in ngrams(file1, 2)])\r\n",
        "ngram_doc2 = ' '.join([' '.join(x) for x in ngrams(file2, 2)])\r\n",
        "ngram_doc3 = ' '.join([' '.join(x) for x in ngrams(file3, 2)])\r\n",
        "ngram_doc4 = ' '.join([' '.join(x) for x in ngrams(file4, 2)])\r\n",
        "ngram_doc5 = ' '.join([' '.join(x) for x in ngrams(file5, 2)])\r\n",
        "\r\n",
        "# documents = [ngram_doc1,ngram_doc2,ngram_doc3,ngram_doc4,ngram_doc5]\r\n",
        "\r\n",
        "documents = [file1,file2,file3,file4,file5]\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer( ngram_range=(2,2)) # TfidfVectorizer has inbuilt ngram kwarg which show tfidf for ngrams\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       quietly extraordinary  of quietly    end of  extraordinary life  \\\n",
            "0                   0.000000    0.000000  0.000000            0.000000   \n",
            "1                   0.000000    0.000000  0.000000            0.000000   \n",
            "2                   0.000000    0.000000  0.000000            0.000000   \n",
            "3                   0.292968    0.292968  0.292968            0.292968   \n",
            "4                   0.000000    0.000000  0.000000            0.000000   \n",
            "Total               0.292968    0.292968  0.292968            0.292968   \n",
            "\n",
            "       family gathered    in and  pneumonia settled    to the  \\\n",
            "0             0.000000  0.000000           0.000000  0.000000   \n",
            "1             0.000000  0.000000           0.000000  0.000000   \n",
            "2             0.000000  0.000000           0.000000  0.174805   \n",
            "3             0.292968  0.292968           0.292968  0.000000   \n",
            "4             0.000000  0.000000           0.000000  0.139807   \n",
            "Total         0.292968  0.292968           0.292968  0.314612   \n",
            "\n",
            "       cornelia vertenstein   the end  \n",
            "0                  0.000000  0.000000  \n",
            "1                  0.197717  0.000000  \n",
            "2                  0.174805  0.000000  \n",
            "3                  0.000000  0.236365  \n",
            "4                  0.000000  0.139807  \n",
            "Total              0.372522  0.376171  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ypv7tb51sPb"
      },
      "source": [
        "**2.Permorfing a spark program to read a dataset and find the W2V similar words (words with higher cosine similarity) for the Top10 TF-IDF Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Mo-blu2KJx"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXiObjEt2eCt"
      },
      "source": [
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, file1),\r\n",
        "        (0.1, file2),\r\n",
        "        (0.2, file3),\r\n",
        "        (0.3, file4),\r\n",
        "        (0.5, file5)\r\n",
        "    ], [\"label\", \"document\"])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpkEgnhT3Usp",
        "outputId": "1cbc44ba-30b8-45a5-ba1b-b37a0f9bab18"
      },
      "source": [
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "print (documentData)\r\n",
        "wordsData.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame[label: double, document: string]\n",
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|The tech giant ha...|[the, tech, giant...|\n",
            "|  0.1|The lessons that ...|[the, lessons, th...|\n",
            "|  0.2|Cornelia Vertenst...|[cornelia, verten...|\n",
            "|  0.3|Pneumonia settled...|[pneumonia, settl...|\n",
            "|  0.5|She began giving ...|[she, began, givi...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX-ji9Tl3uXk"
      },
      "source": [
        "**a.Performing a task without NLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VoSWJGc4Nsb",
        "outputId": "90803c05-fe21-4f90-cef0-098a887a3ac0"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF without NLP:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[1,4,17,33,6...|\n",
            "|  0.1|(200,[15,17,40,50...|\n",
            "|  0.2|(200,[3,5,9,13,15...|\n",
            "|  0.3|(200,[17,23,53,67...|\n",
            "|  0.5|(200,[5,17,40,46,...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF without NLP:\n",
            "Row(label=0.0, document='The tech giant has blocked news to Australians on its platform since last Thursday amid a dispute over a proposed law which would force it and Google to pay news publishers for content.', words=['the', 'tech', 'giant', 'has', 'blocked', 'news', 'to', 'australians', 'on', 'its', 'platform', 'since', 'last', 'thursday', 'amid', 'a', 'dispute', 'over', 'a', 'proposed', 'law', 'which', 'would', 'force', 'it', 'and', 'google', 'to', 'pay', 'news', 'publishers', 'for', 'content.'], rawFeatures=SparseVector(200, {1: 1.0, 4: 1.0, 17: 1.0, 33: 2.0, 64: 1.0, 66: 2.0, 67: 3.0, 74: 1.0, 86: 1.0, 87: 1.0, 88: 2.0, 91: 1.0, 95: 1.0, 97: 1.0, 107: 1.0, 111: 1.0, 129: 1.0, 136: 1.0, 137: 1.0, 144: 1.0, 152: 2.0, 155: 1.0, 167: 1.0, 179: 1.0, 189: 1.0, 191: 1.0, 195: 1.0}), features=SparseVector(200, {1: 1.0986, 4: 1.0986, 17: 0.0, 33: 2.1972, 64: 1.0986, 66: 0.8109, 67: 1.2164, 74: 1.0986, 86: 1.0986, 87: 1.0986, 88: 0.8109, 91: 0.6931, 95: 0.6931, 97: 0.6931, 107: 1.0986, 111: 1.0986, 129: 1.0986, 136: 0.4055, 137: 1.0986, 144: 0.4055, 152: 2.1972, 155: 1.0986, 167: 1.0986, 179: 1.0986, 189: 0.6931, 191: 0.6931, 195: 0.6931}))\n",
            "(200,[1,4,17,33,64,66,67,74,86,87,88,91,95,97,107,111,129,136,137,144,152,155,167,179,189,191,195],[1.0,1.0,1.0,2.0,1.0,2.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.1, document='The lessons that the piano teacher Cornelia Vertenstein taught her students also resounded with many others, including me.', words=['the', 'lessons', 'that', 'the', 'piano', 'teacher', 'cornelia', 'vertenstein', 'taught', 'her', 'students', 'also', 'resounded', 'with', 'many', 'others,', 'including', 'me.'], rawFeatures=SparseVector(200, {15: 1.0, 17: 2.0, 40: 1.0, 50: 1.0, 55: 1.0, 90: 1.0, 108: 1.0, 112: 1.0, 120: 1.0, 123: 1.0, 134: 1.0, 159: 1.0, 160: 1.0, 177: 1.0, 188: 2.0, 192: 1.0}), features=SparseVector(200, {15: 0.6931, 17: 0.0, 40: 0.6931, 50: 1.0986, 55: 1.0986, 90: 0.6931, 108: 1.0986, 112: 0.4055, 120: 1.0986, 123: 1.0986, 134: 0.6931, 159: 1.0986, 160: 1.0986, 177: 1.0986, 188: 1.3863, 192: 1.0986}))\n",
            "(200,[15,17,40,50,55,90,108,112,120,123,134,159,160,177,188,192],[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0])\n",
            "Row(label=0.2, document='Cornelia Vertenstein, a Holocaust survivor, gave her last piano lesson on Feb. She was not feeling well, so she arranged a ride to the hospital.', words=['cornelia', 'vertenstein,', 'a', 'holocaust', 'survivor,', 'gave', 'her', 'last', 'piano', 'lesson', 'on', 'feb.', 'she', 'was', 'not', 'feeling', 'well,', 'so', 'she', 'arranged', 'a', 'ride', 'to', 'the', 'hospital.'], rawFeatures=SparseVector(200, {3: 2.0, 5: 1.0, 9: 1.0, 13: 1.0, 15: 1.0, 17: 1.0, 66: 1.0, 67: 2.0, 78: 1.0, 83: 1.0, 88: 1.0, 90: 1.0, 94: 1.0, 112: 1.0, 134: 1.0, 136: 1.0, 156: 2.0, 157: 1.0, 158: 1.0, 175: 1.0, 189: 1.0, 198: 1.0}), features=SparseVector(200, {3: 2.1972, 5: 0.6931, 9: 1.0986, 13: 1.0986, 15: 0.6931, 17: 0.0, 66: 0.4055, 67: 0.8109, 78: 1.0986, 83: 1.0986, 88: 0.4055, 90: 0.6931, 94: 0.6931, 112: 0.4055, 134: 0.6931, 136: 0.4055, 156: 1.3863, 157: 1.0986, 158: 1.0986, 175: 1.0986, 189: 0.6931, 198: 0.4055}))\n",
            "(200,[3,5,9,13,15,17,66,67,78,83,88,90,94,112,134,136,156,157,158,175,189,198],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.3, document='Pneumonia settled in, and family gathered, sensing the end of a quietly extraordinary life.', words=['pneumonia', 'settled', 'in,', 'and', 'family', 'gathered,', 'sensing', 'the', 'end', 'of', 'a', 'quietly', 'extraordinary', 'life.'], rawFeatures=SparseVector(200, {17: 1.0, 23: 1.0, 53: 1.0, 67: 1.0, 91: 1.0, 95: 1.0, 97: 2.0, 114: 1.0, 130: 1.0, 135: 1.0, 144: 1.0, 182: 1.0, 198: 1.0}), features=SparseVector(200, {17: 0.0, 23: 1.0986, 53: 1.0986, 67: 0.4055, 91: 0.6931, 95: 0.6931, 97: 1.3863, 114: 1.0986, 130: 1.0986, 135: 1.0986, 144: 0.4055, 182: 1.0986, 198: 0.4055}))\n",
            "(200,[17,23,53,67,91,95,97,114,130,135,144,182,198],[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.5, document='She began giving lessons at age in war-torn Romania. She did not stop for nearly years. Toward the end, adapting to the pandemic, Ms. Vertenstein gave lessons on FaceTime from her home in Denver.', words=['she', 'began', 'giving', 'lessons', 'at', 'age', 'in', 'war-torn', 'romania.', 'she', 'did', 'not', 'stop', 'for', 'nearly', 'years.', 'toward', 'the', 'end,', 'adapting', 'to', 'the', 'pandemic,', 'ms.', 'vertenstein', 'gave', 'lessons', 'on', 'facetime', 'from', 'her', 'home', 'in', 'denver.'], rawFeatures=SparseVector(200, {5: 1.0, 17: 2.0, 40: 2.0, 46: 1.0, 63: 2.0, 66: 1.0, 88: 1.0, 94: 1.0, 99: 1.0, 103: 1.0, 112: 1.0, 116: 1.0, 121: 1.0, 132: 1.0, 136: 1.0, 139: 2.0, 144: 1.0, 156: 2.0, 162: 1.0, 165: 1.0, 183: 1.0, 187: 1.0, 188: 1.0, 191: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 198: 1.0}), features=SparseVector(200, {5: 0.6931, 17: 0.0, 40: 1.3863, 46: 1.0986, 63: 2.1972, 66: 0.4055, 88: 0.4055, 94: 0.6931, 99: 1.0986, 103: 1.0986, 112: 0.4055, 116: 1.0986, 121: 1.0986, 132: 1.0986, 136: 0.4055, 139: 2.1972, 144: 0.4055, 156: 1.3863, 162: 1.0986, 165: 1.0986, 183: 1.0986, 187: 1.0986, 188: 0.6931, 191: 0.6931, 193: 1.0986, 194: 1.0986, 195: 0.6931, 196: 1.0986, 198: 0.4055}))\n",
            "(200,[5,17,40,46,63,66,88,94,99,103,112,116,121,132,136,139,144,156,162,165,183,187,188,191,193,194,195,196,198],[1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DfHJ7p14jMP"
      },
      "source": [
        "**b.Performing the task with lemmitization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHjvLc9W5B3i",
        "outputId": "aa4a7ab0-6cae-4910-a072-136eadfda812"
      },
      "source": [
        "import nltk;\r\n",
        "nltk.download('punkt');\r\n",
        "nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "w1 = nltk.word_tokenize(file1)\r\n",
        "w2 = nltk.word_tokenize(file2)\r\n",
        "w3 = nltk.word_tokenize(file3)\r\n",
        "w4 = nltk.word_tokenize(file4)\r\n",
        "w5 = nltk.word_tokenize(file5)\r\n",
        "\r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in w1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in w2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in w3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in w4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in w5])\r\n",
        "\r\n",
        "### lemmatizing words from 5 input docs same as previos task\r\n",
        "\r\n",
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, lemmatized_document1),\r\n",
        "        (0.1, lemmatized_document2),\r\n",
        "        (0.2, lemmatized_document3),\r\n",
        "        (0.3, lemmatized_document4),\r\n",
        "        (0.5, lemmatized_document5)\r\n",
        "    ], [\"label\", \"document\"])\r\n",
        "\r\n",
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "print (documentData)\r\n",
        "wordsData.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "DataFrame[label: double, document: string]\n",
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|The tech giant ha...|[the, tech, giant...|\n",
            "|  0.1|The lesson that t...|[the, lesson, tha...|\n",
            "|  0.2|Cornelia Vertenst...|[cornelia, verten...|\n",
            "|  0.3|Pneumonia settled...|[pneumonia, settl...|\n",
            "|  0.5|She began giving ...|[she, began, givi...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O44b2XSK6o8J",
        "outputId": "dc5ba6f0-45fb-48fb-8441-51f49feeba30"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF with Lemmatization:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[4,5,17,28,3...|\n",
            "|  0.1|(200,[9,15,17,28,...|\n",
            "|  0.2|(200,[3,5,9,13,15...|\n",
            "|  0.3|(200,[17,23,28,53...|\n",
            "|  0.5|(200,[5,9,16,17,2...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF with Lemmatization:\n",
            "Row(label=0.0, document='The tech giant ha blocked news to Australians on it platform since last Thursday amid a dispute over a proposed law which would force it and Google to pay news publisher for content .', words=['the', 'tech', 'giant', 'ha', 'blocked', 'news', 'to', 'australians', 'on', 'it', 'platform', 'since', 'last', 'thursday', 'amid', 'a', 'dispute', 'over', 'a', 'proposed', 'law', 'which', 'would', 'force', 'it', 'and', 'google', 'to', 'pay', 'news', 'publisher', 'for', 'content', '.'], rawFeatures=SparseVector(200, {4: 2.0, 5: 1.0, 17: 1.0, 28: 1.0, 33: 2.0, 38: 1.0, 64: 1.0, 66: 1.0, 67: 2.0, 74: 1.0, 86: 2.0, 87: 1.0, 88: 2.0, 91: 1.0, 95: 1.0, 97: 1.0, 111: 1.0, 129: 1.0, 136: 1.0, 137: 1.0, 144: 1.0, 152: 2.0, 155: 1.0, 167: 1.0, 179: 1.0, 189: 1.0, 191: 1.0, 195: 1.0}), features=SparseVector(200, {4: 2.1972, 5: 0.4055, 17: 0.0, 28: 0.0, 33: 2.1972, 38: 1.0986, 64: 1.0986, 66: 0.4055, 67: 0.0, 74: 1.0986, 86: 2.1972, 87: 1.0986, 88: 0.8109, 91: 0.6931, 95: 0.6931, 97: 0.6931, 111: 1.0986, 129: 1.0986, 136: 0.1823, 137: 1.0986, 144: 0.4055, 152: 2.1972, 155: 1.0986, 167: 1.0986, 179: 1.0986, 189: 0.6931, 191: 0.6931, 195: 0.6931}))\n",
            "(200,[4,5,17,28,33,38,64,66,67,74,86,87,88,91,95,97,111,129,136,137,144,152,155,167,179,189,191,195],[2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.1, document='The lesson that the piano teacher Cornelia Vertenstein taught her student also resounded with many others , including me .', words=['the', 'lesson', 'that', 'the', 'piano', 'teacher', 'cornelia', 'vertenstein', 'taught', 'her', 'student', 'also', 'resounded', 'with', 'many', 'others', ',', 'including', 'me', '.'], rawFeatures=SparseVector(200, {9: 1.0, 15: 1.0, 17: 2.0, 28: 1.0, 50: 1.0, 55: 1.0, 67: 1.0, 90: 1.0, 108: 1.0, 112: 1.0, 134: 1.0, 136: 1.0, 140: 1.0, 159: 1.0, 160: 1.0, 169: 1.0, 188: 2.0, 192: 1.0}), features=SparseVector(200, {9: 0.4055, 15: 0.6931, 17: 0.0, 28: 0.0, 50: 1.0986, 55: 1.0986, 67: 0.0, 90: 0.6931, 108: 1.0986, 112: 0.4055, 134: 1.0986, 136: 0.1823, 140: 1.0986, 159: 1.0986, 160: 1.0986, 169: 1.0986, 188: 0.8109, 192: 1.0986}))\n",
            "(200,[9,15,17,28,50,55,67,90,108,112,134,136,140,159,160,169,188,192],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0])\n",
            "Row(label=0.2, document='Cornelia Vertenstein , a Holocaust survivor , gave her last piano lesson on Feb. She wa not feeling well , so she arranged a ride to the hospital .', words=['cornelia', 'vertenstein', ',', 'a', 'holocaust', 'survivor', ',', 'gave', 'her', 'last', 'piano', 'lesson', 'on', 'feb.', 'she', 'wa', 'not', 'feeling', 'well', ',', 'so', 'she', 'arranged', 'a', 'ride', 'to', 'the', 'hospital', '.'], rawFeatures=SparseVector(200, {3: 1.0, 5: 1.0, 9: 1.0, 13: 1.0, 15: 1.0, 17: 1.0, 28: 1.0, 66: 1.0, 67: 5.0, 70: 1.0, 88: 1.0, 90: 1.0, 94: 1.0, 112: 1.0, 130: 1.0, 135: 1.0, 136: 1.0, 156: 2.0, 157: 2.0, 175: 1.0, 188: 1.0, 189: 1.0, 198: 1.0}), features=SparseVector(200, {3: 1.0986, 5: 0.4055, 9: 0.4055, 13: 1.0986, 15: 0.6931, 17: 0.0, 28: 0.0, 66: 0.4055, 67: 0.0, 70: 1.0986, 88: 0.4055, 90: 0.6931, 94: 0.6931, 112: 0.4055, 130: 1.0986, 135: 1.0986, 136: 0.1823, 156: 1.3863, 157: 2.1972, 175: 1.0986, 188: 0.4055, 189: 0.6931, 198: 0.4055}))\n",
            "(200,[3,5,9,13,15,17,28,66,67,70,88,90,94,112,130,135,136,156,157,175,188,189,198],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.3, document='Pneumonia settled in , and family gathered , sensing the end of a quietly extraordinary life .', words=['pneumonia', 'settled', 'in', ',', 'and', 'family', 'gathered', ',', 'sensing', 'the', 'end', 'of', 'a', 'quietly', 'extraordinary', 'life', '.'], rawFeatures=SparseVector(200, {17: 1.0, 23: 1.0, 28: 1.0, 53: 1.0, 63: 1.0, 67: 3.0, 77: 1.0, 82: 1.0, 91: 1.0, 95: 1.0, 97: 2.0, 144: 1.0, 182: 1.0, 198: 1.0}), features=SparseVector(200, {17: 0.0, 23: 1.0986, 28: 0.0, 53: 0.6931, 63: 0.6931, 67: 0.0, 77: 1.0986, 82: 1.0986, 91: 0.6931, 95: 0.6931, 97: 1.3863, 144: 0.4055, 182: 1.0986, 198: 0.4055}))\n",
            "(200,[17,23,28,53,63,67,77,82,91,95,97,144,182,198],[1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0])\n",
            "Row(label=0.5, document='She began giving lesson at age in war-torn Romania . She did not stop for nearly year . Toward the end , adapting to the pandemic , Ms. Vertenstein gave lesson on FaceTime from her home in Denver .', words=['she', 'began', 'giving', 'lesson', 'at', 'age', 'in', 'war-torn', 'romania', '.', 'she', 'did', 'not', 'stop', 'for', 'nearly', 'year', '.', 'toward', 'the', 'end', ',', 'adapting', 'to', 'the', 'pandemic', ',', 'ms.', 'vertenstein', 'gave', 'lesson', 'on', 'facetime', 'from', 'her', 'home', 'in', 'denver', '.'], rawFeatures=SparseVector(200, {5: 1.0, 9: 2.0, 16: 1.0, 17: 2.0, 28: 3.0, 39: 1.0, 53: 1.0, 63: 2.0, 66: 1.0, 67: 2.0, 88: 1.0, 94: 1.0, 99: 2.0, 103: 1.0, 112: 1.0, 116: 1.0, 121: 1.0, 136: 1.0, 139: 1.0, 144: 1.0, 156: 2.0, 162: 1.0, 165: 1.0, 183: 1.0, 184: 1.0, 188: 1.0, 191: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 198: 1.0}), features=SparseVector(200, {5: 0.4055, 9: 0.8109, 16: 1.0986, 17: 0.0, 28: 0.0, 39: 1.0986, 53: 0.6931, 63: 1.3863, 66: 0.4055, 67: 0.0, 88: 0.4055, 94: 0.6931, 99: 2.1972, 103: 1.0986, 112: 0.4055, 116: 1.0986, 121: 1.0986, 136: 0.1823, 139: 1.0986, 144: 0.4055, 156: 1.3863, 162: 1.0986, 165: 1.0986, 183: 1.0986, 184: 1.0986, 188: 0.4055, 191: 0.6931, 194: 1.0986, 195: 0.6931, 196: 1.0986, 198: 0.4055}))\n",
            "(200,[5,9,16,17,28,39,53,63,66,67,88,94,99,103,112,116,121,136,139,144,156,162,165,183,184,188,191,194,195,196,198],[1.0,2.0,1.0,2.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44uY3XMf6sW0"
      },
      "source": [
        "**c.Performing the task with N-grams data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMtvFRNj7DIV",
        "outputId": "bb3cf947-a7fb-4e47-8e81-ac7b8b1f8738"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, file1.split(' ')),\r\n",
        "        (0.1, file2.split(' ')),\r\n",
        "        (0.2, file3.split(' ')),\r\n",
        "        (0.3, file4.split(' ')),\r\n",
        "        (0.5, file5.split(' '))\r\n",
        "    ], [\"label\", \"document\"])\r\n",
        "\r\n",
        "\r\n",
        "ngram = NGram(n=2, inputCol=\"document\", outputCol=\"ngrams\")\r\n",
        "\r\n",
        "ngramDataFrame = ngram.transform(documentData)\r\n",
        "\r\n",
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(ngramDataFrame)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF with ngram:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[5,7,14,27,2...|\n",
            "|  0.1|(200,[28,38,57,60...|\n",
            "|  0.2|(200,[3,4,5,9,15,...|\n",
            "|  0.3|(200,[25,31,61,95...|\n",
            "|  0.5|(200,[13,14,18,23...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF with ngram:\n",
            "Row(label=0.0, document=['The', 'tech', 'giant', 'has', 'blocked', 'news', 'to', 'Australians', 'on', 'its', 'platform', 'since', 'last', 'Thursday', 'amid', 'a', 'dispute', 'over', 'a', 'proposed', 'law', 'which', 'would', 'force', 'it', 'and', 'Google', 'to', 'pay', 'news', 'publishers', 'for', 'content.'], ngrams=['The tech', 'tech giant', 'giant has', 'has blocked', 'blocked news', 'news to', 'to Australians', 'Australians on', 'on its', 'its platform', 'platform since', 'since last', 'last Thursday', 'Thursday amid', 'amid a', 'a dispute', 'dispute over', 'over a', 'a proposed', 'proposed law', 'law which', 'which would', 'would force', 'force it', 'it and', 'and Google', 'Google to', 'to pay', 'pay news', 'news publishers', 'publishers for', 'for content.'], rawFeatures=SparseVector(200, {5: 1.0, 7: 1.0, 14: 1.0, 27: 1.0, 29: 2.0, 34: 1.0, 41: 1.0, 55: 2.0, 62: 1.0, 68: 1.0, 73: 1.0, 78: 1.0, 82: 1.0, 87: 1.0, 93: 1.0, 97: 1.0, 98: 1.0, 99: 1.0, 103: 1.0, 115: 1.0, 127: 1.0, 134: 1.0, 140: 1.0, 154: 1.0, 163: 1.0, 164: 1.0, 167: 1.0, 184: 1.0, 188: 1.0, 196: 1.0}), features=SparseVector(200, {5: 0.6931, 7: 1.0986, 14: 0.6931, 27: 0.4055, 29: 1.3863, 34: 1.0986, 41: 1.0986, 55: 2.1972, 62: 1.0986, 68: 1.0986, 73: 1.0986, 78: 0.6931, 82: 1.0986, 87: 0.6931, 93: 1.0986, 97: 0.6931, 98: 1.0986, 99: 1.0986, 103: 1.0986, 115: 0.6931, 127: 1.0986, 134: 1.0986, 140: 1.0986, 154: 0.6931, 163: 0.6931, 164: 1.0986, 167: 1.0986, 184: 0.6931, 188: 1.0986, 196: 1.0986}))\n",
            "(200,[5,7,14,27,29,34,41,55,62,68,73,78,82,87,93,97,98,99,103,115,127,134,140,154,163,164,167,184,188,196],[1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.1, document=['The', 'lessons', 'that', 'the', 'piano', 'teacher', 'Cornelia', 'Vertenstein', 'taught', 'her', 'students', 'also', 'resounded', 'with', 'many', 'others,', 'including', 'me.'], ngrams=['The lessons', 'lessons that', 'that the', 'the piano', 'piano teacher', 'teacher Cornelia', 'Cornelia Vertenstein', 'Vertenstein taught', 'taught her', 'her students', 'students also', 'also resounded', 'resounded with', 'with many', 'many others,', 'others, including', 'including me.'], rawFeatures=SparseVector(200, {28: 1.0, 38: 2.0, 57: 1.0, 60: 1.0, 65: 1.0, 69: 1.0, 106: 1.0, 111: 1.0, 124: 1.0, 125: 1.0, 132: 1.0, 145: 1.0, 154: 1.0, 173: 1.0, 193: 1.0, 197: 1.0}), features=SparseVector(200, {28: 1.0986, 38: 2.1972, 57: 1.0986, 60: 0.6931, 65: 1.0986, 69: 1.0986, 106: 0.6931, 111: 0.6931, 124: 1.0986, 125: 1.0986, 132: 1.0986, 145: 1.0986, 154: 0.6931, 173: 1.0986, 193: 1.0986, 197: 1.0986}))\n",
            "(200,[28,38,57,60,65,69,106,111,124,125,132,145,154,173,193,197],[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.2, document=['Cornelia', 'Vertenstein,', 'a', 'Holocaust', 'survivor,', 'gave', 'her', 'last', 'piano', 'lesson', 'on', 'Feb.', 'She', 'was', 'not', 'feeling', 'well,', 'so', 'she', 'arranged', 'a', 'ride', 'to', 'the', 'hospital.'], ngrams=['Cornelia Vertenstein,', 'Vertenstein, a', 'a Holocaust', 'Holocaust survivor,', 'survivor, gave', 'gave her', 'her last', 'last piano', 'piano lesson', 'lesson on', 'on Feb.', 'Feb. She', 'She was', 'was not', 'not feeling', 'feeling well,', 'well, so', 'so she', 'she arranged', 'arranged a', 'a ride', 'ride to', 'to the', 'the hospital.'], rawFeatures=SparseVector(200, {3: 1.0, 4: 1.0, 5: 1.0, 9: 1.0, 15: 1.0, 16: 1.0, 18: 1.0, 23: 1.0, 26: 1.0, 27: 1.0, 29: 1.0, 72: 1.0, 79: 1.0, 84: 1.0, 92: 1.0, 102: 1.0, 106: 1.0, 108: 1.0, 115: 1.0, 119: 1.0, 129: 1.0, 135: 1.0, 148: 1.0, 155: 1.0}), features=SparseVector(200, {3: 1.0986, 4: 1.0986, 5: 0.6931, 9: 1.0986, 15: 1.0986, 16: 1.0986, 18: 0.6931, 23: 0.6931, 26: 1.0986, 27: 0.4055, 29: 0.6931, 72: 1.0986, 79: 1.0986, 84: 0.6931, 92: 1.0986, 102: 1.0986, 106: 0.6931, 108: 1.0986, 115: 0.6931, 119: 1.0986, 129: 0.6931, 135: 0.6931, 148: 1.0986, 155: 1.0986}))\n",
            "(200,[3,4,5,9,15,16,18,23,26,27,29,72,79,84,92,102,106,108,115,119,129,135,148,155],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.3, document=['Pneumonia', 'settled', 'in,', 'and', 'family', 'gathered,', 'sensing', 'the', 'end', 'of', 'a', 'quietly', 'extraordinary', 'life.'], ngrams=['Pneumonia settled', 'settled in,', 'in, and', 'and family', 'family gathered,', 'gathered, sensing', 'sensing the', 'the end', 'end of', 'of a', 'a quietly', 'quietly extraordinary', 'extraordinary life.'], rawFeatures=SparseVector(200, {25: 1.0, 31: 1.0, 61: 1.0, 95: 1.0, 97: 1.0, 105: 1.0, 111: 2.0, 128: 1.0, 129: 1.0, 135: 1.0, 136: 1.0, 163: 1.0}), features=SparseVector(200, {25: 1.0986, 31: 1.0986, 61: 1.0986, 95: 1.0986, 97: 0.6931, 105: 1.0986, 111: 1.3863, 128: 1.0986, 129: 0.6931, 135: 0.6931, 136: 1.0986, 163: 0.6931}))\n",
            "(200,[25,31,61,95,97,105,111,128,129,135,136,163],[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.5, document=['She', 'began', 'giving', 'lessons', 'at', 'age', 'in', 'war-torn', 'Romania.', 'She', 'did', 'not', 'stop', 'for', 'nearly', 'years.', 'Toward', 'the', 'end,', 'adapting', 'to', 'the', 'pandemic,', 'Ms.', 'Vertenstein', 'gave', 'lessons', 'on', 'FaceTime', 'from', 'her', 'home', 'in', 'Denver.'], ngrams=['She began', 'began giving', 'giving lessons', 'lessons at', 'at age', 'age in', 'in war-torn', 'war-torn Romania.', 'Romania. She', 'She did', 'did not', 'not stop', 'stop for', 'for nearly', 'nearly years.', 'years. Toward', 'Toward the', 'the end,', 'end, adapting', 'adapting to', 'to the', 'the pandemic,', 'pandemic, Ms.', 'Ms. Vertenstein', 'Vertenstein gave', 'gave lessons', 'lessons on', 'on FaceTime', 'FaceTime from', 'from her', 'her home', 'home in', 'in Denver.'], rawFeatures=SparseVector(200, {13: 1.0, 14: 1.0, 18: 2.0, 23: 1.0, 24: 1.0, 27: 1.0, 35: 1.0, 39: 1.0, 40: 1.0, 59: 1.0, 60: 1.0, 63: 1.0, 76: 2.0, 78: 1.0, 80: 1.0, 84: 1.0, 87: 1.0, 90: 1.0, 94: 2.0, 101: 1.0, 107: 1.0, 113: 1.0, 137: 1.0, 147: 1.0, 157: 2.0, 159: 1.0, 172: 1.0, 184: 1.0, 187: 1.0}), features=SparseVector(200, {13: 1.0986, 14: 0.6931, 18: 1.3863, 23: 0.6931, 24: 1.0986, 27: 0.4055, 35: 1.0986, 39: 1.0986, 40: 1.0986, 59: 1.0986, 60: 0.6931, 63: 1.0986, 76: 2.1972, 78: 0.6931, 80: 1.0986, 84: 0.6931, 87: 0.6931, 90: 1.0986, 94: 2.1972, 101: 1.0986, 107: 1.0986, 113: 1.0986, 137: 1.0986, 147: 1.0986, 157: 2.1972, 159: 1.0986, 172: 1.0986, 184: 0.6931, 187: 1.0986}))\n",
            "(200,[13,14,18,23,24,27,35,39,40,59,60,63,76,78,80,84,87,90,94,101,107,113,137,147,157,159,172,184,187],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}